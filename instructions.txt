# NeuralChat AI - Ollama Integration Guide

## Overview
This guide will help you integrate Ollama (local LLM) with your NeuralChat AI frontend. Ollama allows you to run large language models locally on your machine, providing privacy and offline capabilities.

## Prerequisites
- Windows 10/11 (or macOS/Linux)
- At least 8GB RAM (16GB+ recommended)
- 10GB+ free disk space
- Modern web browser with JavaScript enabled
- Basic command line knowledge

## Step 1: Install Ollama

### Windows Installation
1. **Download Ollama:**
   - Go to https://ollama.ai/download
   - Click "Download for Windows"
   - Run the installer as administrator

2. **Verify Installation:**
   ```bash
   ollama --version
   ```

### Alternative Installation Methods
```bash
# Using PowerShell (Windows)
winget install Ollama.Ollama

# Using Chocolatey
choco install ollama

# Manual download from GitHub releases
# Visit: https://github.com/ollama/ollama/releases
```

## Step 2: Install Your First Model

### Popular Models to Try
```bash
# Lightweight models (good for testing)
ollama pull llama2:7b
ollama pull mistral:7b-instruct
ollama pull codellama:7b

# Medium models (better quality)
ollama pull llama2:13b
ollama pull mistral:7b-instruct
ollama pull neural-chat:7b

# Large models (best quality, requires more RAM)
ollama pull llama2:70b
ollama pull codellama:34b
```

### Recommended Starting Model
```bash
# Start with this - good balance of speed and quality
ollama pull mistral:7b-instruct
```

## Step 3: Start Ollama Service

### Start Ollama Service
```bash
# Start Ollama (usually runs automatically after installation)
ollama serve
```

**Important:** Keep this terminal window open while using NeuralChat AI.

## Step 4: Test Ollama API

### Test API Endpoint
Open your browser and test the API:
```
http://localhost:11434/api/tags
```

### Test with curl (PowerShell)
```powershell
curl http://localhost:11434/api/tags
```

## Step 5: Setup NeuralChat AI Frontend

### Download/Clone the Repository
```bash
# Clone from GitHub
git clone https://github.com/samkv2/NeuralChat.git
cd NeuralChat

# Or download ZIP from: https://github.com/samkv2/NeuralChat
```

### Start Local Web Server
**CRITICAL:** You must serve the files through a web server (not open directly) due to CORS restrictions.

```bash
# Option 1: Python (recommended)
python -m http.server 8000

# Option 2: Node.js
npx http-server -p 8000

# Option 3: PHP
php -S localhost:8000
```

### Open the Application
Navigate to: `http://localhost:8000`

## Step 6: Verify Integration

### Check Connection Status
1. **Status Indicator**: Should show "Ollama Connected" (green dot)
2. **Model Dropdown**: Should display your installed models with ðŸ¤– emoji
3. **Console Logs**: Press F12 â†’ Console tab to see connection status

### Test Chat Functionality
1. Select a model from the dropdown (e.g., `ðŸ¤– mistral:7b-instruct`)
2. Type a message: "Hello, how are you?"
3. Press Enter or click Send
4. Wait for AI response

## Step 7: Troubleshooting

### Common Issues and Solutions

#### 1. "Ollama Offline" Status
**Problem:** Status shows red dot "Ollama Offline"
**Solutions:**
- Make sure `ollama serve` is running
- Check if port 11434 is available: `netstat -an | findstr 11434`
- Restart Ollama service
- Use the refresh button (ðŸ”„) in the interface

#### 2. No Models in Dropdown
**Problem:** Model dropdown is empty or doesn't show Ollama models
**Solutions:**
- Verify models are installed: `ollama list`
- Check Ollama is running: `ollama serve`
- Refresh the page or click the refresh button
- Check browser console for errors (F12)

#### 3. CORS Errors
**Problem:** Browser shows CORS errors in console
**Solutions:**
- **CRITICAL:** Always use a web server (not file:// protocol)
- Use `python -m http.server 8000` or similar
- Access via `http://localhost:8000` not `file://`

#### 4. "AI is thinking..." Stuck
**Problem:** Typing indicator doesn't disappear
**Solutions:**
- This was fixed in the latest version
- Refresh the page if it happens
- Check browser console for errors

#### 5. Slow Responses
**Problem:** AI responses are very slow
**Solutions:**
- Use smaller models (7B instead of 13B/70B)
- Close other applications to free up RAM
- Consider using quantized models
- Check system resources

### Debug Commands
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# List installed models
ollama list

# Test a simple generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "mistral:7b-instruct", "prompt": "Hello!", "stream": false}'

# Check port usage
netstat -an | findstr 11434
```

## Step 8: Advanced Configuration

### Model Management
The application automatically detects and loads all your Ollama models. Supported formats:
- `mistral:7b-instruct`
- `llama2:7b`
- `codellama:7b`
- Any Ollama-compatible model

### Performance Optimization
- **Start with 7B models** for faster responses
- **Monitor RAM usage** (models can use 4-8GB+ RAM)
- **Use quantized models** for better performance
- **Close unnecessary applications** while using AI

### Customization
You can modify the following in `script.js`:
- Default model selection
- API timeout settings
- Response parameters (temperature, top_p, etc.)
- UI behavior and animations

## Step 9: Production Deployment

### For Production Use
1. **Security:** Add authentication if needed
2. **Scaling:** Consider Ollama server deployment
3. **Monitoring:** Add logging and error tracking
4. **Backup:** Regular model and configuration backups

### Environment Variables
```bash
# Set custom Ollama URL
export OLLAMA_HOST=0.0.0.0:11434

# Set model directory
export OLLAMA_MODELS=/path/to/models
```

## Step 10: Features Overview

### Current Features
- âœ… **Real-time Chat**: Live typing indicators and smooth animations
- âœ… **Model Detection**: Automatic detection of installed Ollama models
- âœ… **Glassmorphism Design**: Modern UI with neural network animations
- âœ… **Responsive Design**: Works on desktop, tablet, and mobile
- âœ… **Error Handling**: Comprehensive error handling and user feedback
- âœ… **Connection Status**: Real-time connection monitoring
- âœ… **Refresh Button**: Manual connection refresh capability

### Technical Features
- **CORS Handling**: Proper cross-origin request handling
- **Async/Await**: Modern JavaScript for smooth user experience
- **Dynamic UI**: Real-time model loading and status updates
- **Debug Logging**: Comprehensive console logging for troubleshooting

## Additional Resources

- **NeuralChat Repository:** https://github.com/samkv2/NeuralChat
- **Ollama Documentation:** https://ollama.ai/docs
- **Model Library:** https://ollama.ai/library
- **GitHub Repository:** https://github.com/ollama/ollama
- **Community:** https://github.com/ollama/ollama/discussions

## Support

If you encounter issues:
1. Check Ollama logs: `ollama logs`
2. Restart Ollama service: `ollama serve`
3. Check available models: `ollama list`
4. Verify API endpoint: `curl http://localhost:11434/api/tags`
5. Check browser console (F12) for JavaScript errors
6. Ensure you're using a web server (not file:// protocol)

## Quick Start Checklist

- [ ] Ollama installed and running (`ollama serve`)
- [ ] At least one model downloaded (`ollama pull mistral:7b-instruct`)
- [ ] NeuralChat AI files downloaded/cloned
- [ ] Local web server running (`python -m http.server 8000`)
- [ ] Application opened in browser (`http://localhost:8000`)
- [ ] Status shows "Ollama Connected" (green dot)
- [ ] Models visible in dropdown with ðŸ¤– emoji
- [ ] Chat functionality working

---

**Happy chatting with your local AI! ðŸ¤–âœ¨**

**Repository:** https://github.com/samkv2/NeuralChat
